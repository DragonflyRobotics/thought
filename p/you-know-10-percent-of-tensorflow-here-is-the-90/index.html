<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="You haven‚Äôt used TensorFlow until you have used this! How to use the full power of TensorFlow to make POWERFUL algorithms!"><title>You know 10% of TensorFlow; Here is the 90%</title><link rel=canonical href=https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/><link rel=stylesheet href=/scss/style.min.833d6eed45de56f48306bf57268d5b8cdfc8a60e8e7bdc99810464fcd033f7c6.css><meta property='og:title' content="You know 10% of TensorFlow; Here is the 90%"><meta property='og:description' content="You haven‚Äôt used TensorFlow until you have used this! How to use the full power of TensorFlow to make POWERFUL algorithms!"><meta property='og:url' content='https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/'><meta property='og:site_name' content='Thoughts From a Dragonfly'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Tensorflow'><meta property='article:tag' content='AI'><meta property='article:tag' content='Python'><meta property='article:published_time' content='2023-06-10T00:00:00+00:00'><meta property='article:modified_time' content='2023-06-10T00:00:00+00:00'><meta property='og:image' content='https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/cover.webp'><meta name=twitter:title content="You know 10% of TensorFlow; Here is the 90%"><meta name=twitter:description content="You haven‚Äôt used TensorFlow until you have used this! How to use the full power of TensorFlow to make POWERFUL algorithms!"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/cover.webp'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_3706edba391e6d38.png width=300 height=321 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üß†</span></figure><div class=site-meta><h1 class=site-name><a href=/>Thoughts From a Dragonfly</a></h1><h2 class=site-description>Premier place to hear the thoughts of a mere Dragonfly who learned to outlive the dinosaurs.</h2></div></header><ol class=menu-social><li><a href=https://github.com/DragonflyRobotics target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://krishnashah.dev target=_blank title=Home rel=me><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#how-we-currently-use-tensorflow>How we currently use TensorFlow?</a></li><li><a href=#math>Math???</a></li><li><a href=#gradienttape>GradientTape</a></li><li><a href=#loss-function>Loss Function</a><ol><li><a href=#how-to-write-a-custom-loss-function>How to write a custom loss function?</a></li></ol></li><li><a href=#optimization>Optimization</a><ol><li><a href=#adam>Adam</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/you-know-10-percent-of-tensorflow-here-is-the-90/><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/cover_hu_233f178f29acc68a.webp srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/cover_hu_233f178f29acc68a.webp 800w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/cover_hu_378b73f734ca064d.webp 1600w" width=800 height=450 loading=lazy alt="Featured image of post You know 10% of TensorFlow; Here is the 90%"></a></div><div class=article-details><header class=article-category><a href=/categories/tensorflow/>Tensorflow
</a><a href=/categories/ai/>AI
</a><a href=/categories/python/>Python</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/you-know-10-percent-of-tensorflow-here-is-the-90/>You know 10% of TensorFlow; Here is the 90%</a></h2><h3 class=article-subtitle>You haven‚Äôt used TensorFlow until you have used this! How to use the full power of TensorFlow to make POWERFUL algorithms!</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2023-06-10T00:00:00Z>Jun 10, 2023</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>Most people who have worked with TensorFlow have used the Keras API. The Keras API was acquired by Google and added to TensorFlow to simplify Neural Network development by creating an easy and systematic process to train, test, and evaluate neural networks. This is, however, the tip of the iceberg! Stick around until the very end as I open doors to a new, powerful world of custom algorithms, gradients, and losses.</p><p>This article will be divided into three distinct sections: Custom Training, Custom Loss, and Custom Forward Passes!</p><h2 id=how-we-currently-use-tensorflow>How we currently use TensorFlow?</h2><p>Currently, TensorFlow is used as follows: load data, make a Sequential network, select an optimizer and loss function, and train! As simple and convenient as that sounds, it is limited to basic, ‚Äúoff-the-shelf‚Äù networks. Here is a basic example from TensorFlow:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;TensorFlow version:&#34;</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mnist</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>mnist</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>),</span> <span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span> <span class=o>=</span> <span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span> <span class=o>=</span> <span class=n>x_train</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>,</span> <span class=n>x_test</span> <span class=o>/</span> <span class=mf>255.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(</span><span class=n>from_logits</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>loss</span><span class=o>=</span><span class=n>loss_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span>  <span class=n>y_test</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>As you will see, every component of this program can be tweaked, expanded, and customized!</p><h2 id=math>Math???</h2><p>One more thing: We need to learn some basics about TensorFlow mathematics! To understand why this is so significant ‚Äî arguably the most crucial feature of TensorFlow ‚Äî you must first understand the basics of multivariate calculus.</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/1.webp width=357 height=698 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/1_hu_bd71bebbc35c740e.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/1_hu_3eba5d3f74ef80cc.webp 1024w" loading=lazy class=gallery-image data-flex-grow=51 data-flex-basis=122px></p><p>Without delving into the inner complexities of the algorithm depicted above, this represents what an average Dense neuron does during forward and backward propagation. One thing to note is that I have depicted gradient descent as a scaled partial derivative. In reality, an optimizer function is used that utilizes momentum and stochastic gradient descent for higher accuracy and faster convergence. This process can be modified in many locations, including the forward pass, loss function, and optimizer to suit the needs of the task at hand.
Custom Training</p><p>Despite its convoluted fa√ßade, TensorFlow makes this relatively simple. During training, there are three primary steps:</p><ol><li>Forward pass</li><li>Calculate loss</li><li>Calculate/propagate gradients.</li></ol><p>Let‚Äôs create a simple example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Start of epoch </span><span class=si>%d</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>epoch</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>step</span><span class=p>,</span> <span class=p>(</span><span class=n>x_batch_train</span><span class=p>,</span> <span class=n>y_batch_train</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_batch_train</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss_value</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>y_batch_train</span><span class=p>,</span> <span class=n>logits</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>grads</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>loss_value</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>trainable_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>apply_gradients</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>trainable_weights</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=mi>200</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;Training loss (for one batch) at step </span><span class=si>%d</span><span class=s2>: </span><span class=si>%.4f</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=o>%</span> <span class=p>(</span><span class=n>step</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=n>loss_value</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Seen so far: </span><span class=si>%s</span><span class=s2> samples&#34;</span> <span class=o>%</span> <span class=p>((</span><span class=n>step</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>batch_size</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Firstly, we iterate over the number of epochs the algorithm needs to run for. Next, we must iterate over the entire dataset for a single training step. Now to the more important step: GradientTape. This is a way for TensorFlow to watch variables for change and use automatic (reverse mode) differentiation to find the partial derivatives.</p><h2 id=gradienttape>GradientTape</h2><p>This function watches all <code>tf.Variable</code> or TF logits for change (in terms of mathematical operations performed on it. It then creates a graph of all the functions performed on to as such:</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/2.webp width=1042 height=754 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/2_hu_919cbdf44f26bd88.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/2_hu_ed2ad490c5187adf.webp 1024w" loading=lazy class=gallery-image data-flex-grow=138 data-flex-basis=331px></p><p>It then calculates the partial derivative with respect to each step before it. Finally, we multiply the partials from the nodes of each path and sum all the products from all the paths that lead to the same source. For example, let‚Äôs find the ‚àÇd/‚àÇa.</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/3.webp width=640 height=167 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/3_hu_db7acc93ea30f111.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/3_hu_36f344ac6500d455.webp 1024w" loading=lazy class=gallery-image data-flex-grow=383 data-flex-basis=919px></p><p>This is a much more powerful and robust approach than simply finding the change <code>d</code> with respect to <code>a</code> numerically using <em>numerical (discrete) differentiation</em>. This is because the latter approach is prone to <em>divide-by-zero errors</em>.</p><p>Let‚Äôs use a simple, practical demo! Let‚Äôs try to numerically and automatically differentiate the following function:</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/4.webp width=602 height=84 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/4_hu_c84bb22aeb96bb6e.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/4_hu_f115e75b63d51989.webp 1024w" loading=lazy class=gallery-image data-flex-grow=716 data-flex-basis=1720px></p><p>Symbolically differentiation (using the power rule), we would derive the following:</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/5.webp width=580 height=97 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/5_hu_730ec8d78471408f.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/5_hu_35eb51b36ce10966.webp 1024w" loading=lazy class=gallery-image data-flex-grow=597 data-flex-basis=1435px></p><p>To test this, we will evaluate the gradient of this function at the point <strong>(4, 156)</strong>:</p><p><img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/6.webp width=621 height=335 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/6_hu_1722c9fa4c27fed4.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/6_hu_9b33a158582730b8.webp 1024w" loading=lazy class=gallery-image data-flex-grow=185 data-flex-basis=444px></p><p>Now, let us write a <code>GradientTape</code> function with TensorFlow:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>f_x</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl> <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>**</span> <span class=mi>3</span><span class=p>)</span> <span class=o>+</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>4.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>tape</span><span class=o>.</span><span class=n>watch</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>y</span> <span class=o>=</span> <span class=n>f_x</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>====================================</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>tf</span><span class=o>.</span><span class=n>Tensor</span><span class=p>(</span><span class=mf>102.0</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>float32</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>An important thing to consider here is how I was required to add the <code>tape.watch(x)</code> method. This is because GradientTape only automatically watches <code>tf.Variable</code> and logits from TF NN and TF Losses. You must convert it to a <code>tf.constant</code> and manually watch any other variables.</p><h2 id=loss-function>Loss Function</h2><p>The next impressive feature TensorFlow provides is the ability to make custom loss functions. While this seems obvious, it cannot be understated. Determining if the neural network is failing or succeeding at its tasks, it serves as a teacher/mentor of the network by influencing the gradients and allowing the correction to propagate through the network. This can be done quite simply:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=nd>@tf.function</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>euclidean</span><span class=p>(</span><span class=n>y1</span><span class=p>,</span> <span class=n>y2</span><span class=p>,</span> <span class=n>T</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>convert_to_tensor</span><span class=p>(</span><span class=n>y1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;float32&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>y2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>convert_to_tensor</span><span class=p>(</span><span class=n>y2</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;float32&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>y1</span><span class=p>,</span> <span class=n>y2</span><span class=p>)),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>dist</span><span class=p>,</span> <span class=n>T</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Simply writing a function and decorating it with the <code>tf.function</code> function can be enough to make a powerful TF function. The decorator allows TensorFlow to create graphs (as shown in the GradientTape section) which is necessary to save the model and increase performance.</p><p>Another way to write a simple loss function is through the tf.keras.losses.Loss abstract class. While this is a convenient, more robust way to make a loss function, it might have some limitations imposed by the abstract class for standardization.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=k>class</span> <span class=nc>MeanSquaredError</span><span class=p>(</span><span class=n>Loss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>call</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>math</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>),</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=how-to-write-a-custom-loss-function>How to write a custom loss function?</h3><p>Making a loss function is more an art than an exact science. It involves the following preliminary steps:</p><ol><li>Gather all the ‚Äúmetric‚Äù requirements you want your neural network to fulfill.</li><li>Formulate a way to evaluate the metrics numerically.</li><li>Using direct/inverse relationships, combine these metrics into a single mathematical loss function.</li></ol><p>One of the best examples of this strategy is the Contrastive Loss Function used in many modern networks like the Siamese network.
<img src=/p/you-know-10-percent-of-tensorflow-here-is-the-90/7.webp width=700 height=77 srcset="/p/you-know-10-percent-of-tensorflow-here-is-the-90/7_hu_5cd3e47520c46cd6.webp 480w, /p/you-know-10-percent-of-tensorflow-here-is-the-90/7_hu_3857275f1352067b.webp 1024w" loading=lazy class=gallery-image data-flex-grow=909 data-flex-basis=2181px></p><p>D_w ‚Äî Distance between points</p><p>Y ‚Äî Truth value (0 = not related; 1 = related)</p><p>This loss function is meant to calculate the distance between two tensors for image differentiation. It has two terms: one is responsible for rewarding the algorithm for having points closer together, while the other returns 0 if the points are farther out. This is made more apparent when the 1/2 normalization terms are removed.</p><h2 id=optimization>Optimization</h2><p>This is the final major way to customize your AI adventures. As explained earlier, the backpropagation stage in the <strong>Math???</strong> section was simplified to a linear function with the learning rate scaling the gradient. There are <strong>much</strong> more powerful techniques to get more performant algorithms with faster convergence and higher accuracy.</p><h3 id=adam>Adam</h3><p>Almost all beginners have heard of the Adam optimizer. It is, after all, the most popular optimizer for a wide variety of DNN tasks. This is the perfect example of a momentum-based optimizer with squared gradients. Firstly, it must square (and then square root) the gradients. This reduces a lot of the oscillation and removes all sense of magnitude from the gradients leaving just the magnitude of the step. Next, it will calculate a running average of the gradients to prevent erratic changes in the concurrent gradients and magnify the gradients if the loss is rapidly converging.</p><h2 id=conclusion>Conclusion</h2><p>As I have (hopefully) demonstrated in this article, a vast universe of possibilities fuels modern deep-learning research and development. Whether you use one or more of the techniques listed in this article; whether you use TensorFlow, PyTorch, Julia, etc.; there are a lot of variables meant to be customized to advance development.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/tensorflow/>Tensorflow</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/python/>Python</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/what-is-a-siamese-network/><div class=article-image><img src=/p/what-is-a-siamese-network/cover.1793060cbc7e3bb92a4aaf2641cec6c1_hu_50f9667816234dab.webp width=250 height=150 loading=lazy alt="Featured image of post What is a Siamese Network? (Implementation Included)" data-key=what-is-a-siamese-network data-hash="md5-F5MGDLx+O7kqSq8mQc7GwQ=="></div><div class=article-details><h2 class=article-title>What is a Siamese Network? (Implementation Included)</h2></div></a></article><article class=has-image><a href=/p/what-really-is-general-intelligence/><div class=article-image><img src=/p/what-really-is-general-intelligence/cover.2d6b802d3fa1aed3aa611a204c8fd2dd_hu_ea445bd25a903e35.webp width=250 height=150 loading=lazy alt="Featured image of post What REALLY is General Intelligence?" data-key=what-really-is-general-intelligence data-hash="md5-LWuALT+hrtOqYRogTI/S3Q=="></div><div class=article-details><h2 class=article-title>What REALLY is General Intelligence?</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Thoughts From a Dragonfly</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>