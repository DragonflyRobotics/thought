[{"content":"\nKnowledge is a goal, a goal impossible to reach. A goal scholar pursues their entire existence, knowing its unfeasibility. It is the peak of the highest mountain that mountaineers, weak and strong alike, climb, knowing their grave lies amidst the snow. It is a thought that intoxicates all who see its value and allows them to persevere through the climb to conquer more of the mountain and accept their ascertained defeat.\nKnowledge is the pinnacle of thought and understanding of everything that is to be understood. Knowledge about the small atoms that work in unison to produce marvelous chemical machines, to the grand universe of unfathomable complexity. Knowledge about the predictable day after a long night to the stochasticity of the clouds encompassing our globe.\nKnowledge is a key. A key that unlocks opportunity itself. From a simple job interview to the cure for cancer, it holds the very secret to things humans value most. A small, golden key in a world made of locks. Knowledge is key to every job, family, conversation, culture, achievement, and goal humans can think of, yet it remains a goal impossible to achieve.\nKnowledge is a beacon that illuminates the path to all human desires. A beacon so bright as to irradiate outer space and the abyssal depths of the ocean. It is a tool that can show an enlightening path to peace and prosperity and the inhumane weapons of war. A beacon that — not only sheds light on the path ahead — but also illuminates the characteristics of the person holding the torch.\nAlthough some might say that knowledge is attainable in a topic of smaller scope, to say that a subset of an infinite expanse of knowledge is finite would be to say that the human race has perfected certain fields, and nothing new can be obtained from them. A complete understanding of a certain field would require a complete understanding of knowledge itself since every component of this universe is linked — often in subtle ways.\nKnowledge is a question as old as time itself. It materializes in many forms, from the inherent order in nature to “Cogito, ergo sum.” It is an existential question, unanswerable by even the greatest intellect. It is a question that trumps all questions in comparison, whose answer unlocks the very mysteries of nature itself.\nWhile some think they have attained knowledge, and many more believe it is attainable in a human lifespan, we must consider the minuscule role we play in the infinite expanse of our universe.\n","date":"2023-12-04T00:00:00Z","image":"https://thought.krishnashah.dev/p/knowledge-a-societal-driving-force/cover_hu_d551ef187e3bf0f1.webp","permalink":"https://thought.krishnashah.dev/p/knowledge-a-societal-driving-force/","title":"Knowledge — A Societal Driving Force"},{"content":"Introduction Most people who have worked with TensorFlow have used the Keras API. The Keras API was acquired by Google and added to TensorFlow to simplify Neural Network development by creating an easy and systematic process to train, test, and evaluate neural networks. This is, however, the tip of the iceberg! Stick around until the very end as I open doors to a new, powerful world of custom algorithms, gradients, and losses.\nThis article will be divided into three distinct sections: Custom Training, Custom Loss, and Custom Forward Passes!\nHow we currently use TensorFlow? Currently, TensorFlow is used as follows: load data, make a Sequential network, select an optimizer and loss function, and train! As simple and convenient as that sounds, it is limited to basic, “off-the-shelf” networks. Here is a basic example from TensorFlow:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import tensorflow as tf print(\u0026#34;TensorFlow version:\u0026#34;, tf.__version__) mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10) ]) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test, verbose=2) As you will see, every component of this program can be tweaked, expanded, and customized!\nMath??? One more thing: We need to learn some basics about TensorFlow mathematics! To understand why this is so significant — arguably the most crucial feature of TensorFlow — you must first understand the basics of multivariate calculus.\nWithout delving into the inner complexities of the algorithm depicted above, this represents what an average Dense neuron does during forward and backward propagation. One thing to note is that I have depicted gradient descent as a scaled partial derivative. In reality, an optimizer function is used that utilizes momentum and stochastic gradient descent for higher accuracy and faster convergence. This process can be modified in many locations, including the forward pass, loss function, and optimizer to suit the needs of the task at hand. Custom Training\nDespite its convoluted façade, TensorFlow makes this relatively simple. During training, there are three primary steps:\nForward pass Calculate loss Calculate/propagate gradients. Let’s create a simple example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 epochs = 2 for epoch in range(epochs): print(\u0026#34;\\nStart of epoch %d\u0026#34; % (epoch,)) for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x_batch_train, training=True) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) if step % 200 == 0: print( \u0026#34;Training loss (for one batch) at step %d: %.4f\u0026#34; % (step, float(loss_value)) ) print(\u0026#34;Seen so far: %s samples\u0026#34; % ((step + 1) * batch_size)) Firstly, we iterate over the number of epochs the algorithm needs to run for. Next, we must iterate over the entire dataset for a single training step. Now to the more important step: GradientTape. This is a way for TensorFlow to watch variables for change and use automatic (reverse mode) differentiation to find the partial derivatives.\nGradientTape This function watches all tf.Variable or TF logits for change (in terms of mathematical operations performed on it. It then creates a graph of all the functions performed on to as such:\nIt then calculates the partial derivative with respect to each step before it. Finally, we multiply the partials from the nodes of each path and sum all the products from all the paths that lead to the same source. For example, let’s find the ∂d/∂a.\nThis is a much more powerful and robust approach than simply finding the change d with respect to a numerically using numerical (discrete) differentiation. This is because the latter approach is prone to divide-by-zero errors.\nLet’s use a simple, practical demo! Let’s try to numerically and automatically differentiate the following function:\nSymbolically differentiation (using the power rule), we would derive the following:\nTo test this, we will evaluate the gradient of this function at the point (4, 156):\nNow, let us write a GradientTape function with TensorFlow:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import tensorflow as tf def f_x(x): return 2 * (x ** 3) + 6 * x + 4 with tf.GradientTape() as tape: x = tf.constant(4.0) tape.watch(x) y = f_x(x) print(tape.gradient(y, x)) ==================================== \u0026gt;\u0026gt;\u0026gt; tf.Tensor(102.0, shape=(), dtype=float32) An important thing to consider here is how I was required to add the tape.watch(x) method. This is because GradientTape only automatically watches tf.Variable and logits from TF NN and TF Losses. You must convert it to a tf.constant and manually watch any other variables.\nLoss Function The next impressive feature TensorFlow provides is the ability to make custom loss functions. While this seems obvious, it cannot be understated. Determining if the neural network is failing or succeeding at its tasks, it serves as a teacher/mentor of the network by influencing the gradients and allowing the correction to propagate through the network. This can be done quite simply:\n1 2 3 4 5 6 @tf.function def euclidean(y1, y2, T): y1 = tf.squeeze(tf.convert_to_tensor(y1, dtype=\u0026#39;float32\u0026#39;)) y2 = tf.squeeze(tf.convert_to_tensor(y2, dtype=\u0026#39;float32\u0026#39;)) dist = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(y1, y2)), axis=0)) return tf.square(tf.subtract(dist, T)) Simply writing a function and decorating it with the tf.function function can be enough to make a powerful TF function. The decorator allows TensorFlow to create graphs (as shown in the GradientTape section) which is necessary to save the model and increase performance.\nAnother way to write a simple loss function is through the tf.keras.losses.Loss abstract class. While this is a convenient, more robust way to make a loss function, it might have some limitations imposed by the abstract class for standardization.\n1 2 3 4 class MeanSquaredError(Loss): def call(self, y_true, y_pred): return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1) How to write a custom loss function? Making a loss function is more an art than an exact science. It involves the following preliminary steps:\nGather all the “metric” requirements you want your neural network to fulfill. Formulate a way to evaluate the metrics numerically. Using direct/inverse relationships, combine these metrics into a single mathematical loss function. One of the best examples of this strategy is the Contrastive Loss Function used in many modern networks like the Siamese network. D_w — Distance between points\nY — Truth value (0 = not related; 1 = related)\nThis loss function is meant to calculate the distance between two tensors for image differentiation. It has two terms: one is responsible for rewarding the algorithm for having points closer together, while the other returns 0 if the points are farther out. This is made more apparent when the 1/2 normalization terms are removed.\nOptimization This is the final major way to customize your AI adventures. As explained earlier, the backpropagation stage in the Math??? section was simplified to a linear function with the learning rate scaling the gradient. There are much more powerful techniques to get more performant algorithms with faster convergence and higher accuracy.\nAdam Almost all beginners have heard of the Adam optimizer. It is, after all, the most popular optimizer for a wide variety of DNN tasks. This is the perfect example of a momentum-based optimizer with squared gradients. Firstly, it must square (and then square root) the gradients. This reduces a lot of the oscillation and removes all sense of magnitude from the gradients leaving just the magnitude of the step. Next, it will calculate a running average of the gradients to prevent erratic changes in the concurrent gradients and magnify the gradients if the loss is rapidly converging.\nConclusion As I have (hopefully) demonstrated in this article, a vast universe of possibilities fuels modern deep-learning research and development. Whether you use one or more of the techniques listed in this article; whether you use TensorFlow, PyTorch, Julia, etc.; there are a lot of variables meant to be customized to advance development.\n","date":"2023-06-10T00:00:00Z","image":"https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/cover_hu_868e4ec19c06e0af.webp","permalink":"https://thought.krishnashah.dev/p/you-know-10-percent-of-tensorflow-here-is-the-90/","title":"You know 10% of TensorFlow; Here is the 90%"},{"content":"Introduction Chances are you have heard of a convolutional neural network (CNN). A network that revolves a kernel around an image to extract features and spatially interpret the image. These networks are some of the most prominent networks in the AI Industry, with applications in object detection, image processing, feature extraction, background removal, and image reconstruction and generation. With a minor but ingenious modification, these relatively primitive networks can be capable of an entirely novel niche of deep learning tasks enabling some powerful modern technologies like biometric authentication.\nWhat is the architecture of a Siamese Network? The architecture of a Siamese Network consists of two convolutional neural networks that run in parallel on two different images. Furthermore, these two CNNs have identical weights and biases, allowing them to use similar extracted features to analyze differently.\nWe value two separate images, ground truth, and an “imposter”, with the same feature extractors and loss function to return an arbitrary similarity unit. To understand this concept further, it is imperative to understand the loss function governing the network.\nLoss Function These networks typically use one of two loss functions: Contrastive Loss and Euclidean Loss.\nEuclidean Loss y = one of the CNN outputs\ny_hat = the other CNN output\nT = ground truth value (array of 1s or 0s)\nThis is the classic example of the “distance formula” applied to higher dimensions through matrices. We take a point in a high-dimension plane (i.e., the size of y and y_hat) and compute their Euclidean distance. This is a simple — but effective — technique to find the distance.\nContrastive Loss D_w — Distance between points\nY — Truth value (0 = not related; 1 = related)\nThis loss function has two terms: one is responsible for rewarding the algorithm for having points closer together, while the other returns 0 if the points are farther out. This is made more apparent when the 1/2 normalization terms are removed.\nImplementation With the theory out of the way, it is time for practical implementation. This implementation will be done in Tensorflow, but the PyTorch implementation should be similar in concept.\nThe first task is to import libraries and do the initial setup. My setup consists of a Data folder with subfolders for each of the categories.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import os import pathlib import random import tensorflow as tf print(\u0026#34;TensorFlow version:\u0026#34;, tf.__version__) from tensorflow.keras.layers import Dense, Flatten, Conv2D AUTOTUNE = tf.data.AUTOTUNE dirs = os.listdir(str(pathlib.Path(\u0026#34;Data\u0026#34;).resolve())) train_ds_arr = [] BATCH_SIZE = 32 Next, we need to create the class that will define the layers of the network and the forward pass.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class SiameseNet(tf.keras.models.Model): def __init__(self): super(SiameseNet, self).__init__() self.conv1 = Conv2D(64, 3, activation=\u0026#39;relu\u0026#39;) self.conv2 = Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;) self.conv3 = Conv2D(64, 3, activation=\u0026#39;relu\u0026#39;) self.flatten = Flatten() self.d1 = Dense(128, activation=\u0026#39;relu\u0026#39;) self.d2 = Dense(64, activation=\u0026#39;relu\u0026#39;) self.d3 = Dense(32, ) def call(self, x1, x2): # x1 = self.conv1(x1) # x1 = self.conv2(x1) x1 = self.conv3(x1) x1 = self.flatten(x1) x1 = self.d1(x1) x1 = self.d2(x1) x1 = self.d3(x1) # x2 = self.conv1(x2) # x2 = self.conv2(x2) x2 = self.conv3(x2) x2 = self.flatten(x2) x2 = self.d1(x2) x2 = self.d2(x2) x2 = self.d3(x2) return x1, x2 I have commented on the first two layers for computational simplicity (and image size limitations), but they can be used if needed. Additionally, the forward pass requires two arguments (namely x1 and x2). This is because we use the same layers, thus the same weights and biases, in the forward pass of both images.\nWe can now make an object of the class and initialize the optimizer.\n1 2 snn = SiameseNet() optimizer = tf.keras.optimizers.Adam() We must now create three functions to process the data and generate training batches.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def get_label(file_path): parts = tf.strings.split(file_path, os.path.sep) one_hot = parts[-2] == dirs return tf.argmax(one_hot) def decode_img(img): img = tf.io.decode_jpeg(img, channels=3) return tf.image.resize(img, [170, 170]) def process_path(file_path): label = get_label(file_path) img = tf.io.read_file(file_path) img = decode_img(img) return img, label The get_label function is responsible for using the file path to create an integer label for each category in the Data folder. The decode_img function is responsible for loading an image into TensorFlow and resizing it. Finally, the process_path function takes a given path, reads the file, decodes the image, and returns the modified image with its label.\nNext, we must create TensorFlow datasets of the images for each directory and create a parent array to hold them.\n1 2 3 4 5 6 for d in dirs: data_dir = pathlib.Path(os.path.join(\u0026#34;Data\u0026#34;, d)).with_suffix(\u0026#39;\u0026#39;) x = tf.data.Dataset.list_files(str(data_dir / \u0026#39;*\u0026#39;), shuffle=True) x = x.map(process_path, num_parallel_calls=AUTOTUNE) train_ds_arr.append(x) Here, we map the listed files using out process_path function to create a dataset with images and labels.\nWe can now compile our model, set up metrics, and initialize callbacks.\n1 2 3 4 train_loss = tf.keras.metrics.Mean(name=\u0026#39;train_loss\u0026#39;) tb_callback = tf.keras.callbacks.TensorBoard(\u0026#34;tensorboard\u0026#34;) tb_callback.set_model(snn) Most importantly, we need to create a loss function I discussed earlier. For the sake of simplicity, I will use the Euclidean loss:\n1 2 3 4 5 6 7 8 9 @tf.function def euclidean(y1, y2, T): y1 = tf.convert_to_tensor(y1, dtype=\u0026#39;float32\u0026#39;) y2 = tf.convert_to_tensor(y2, dtype=\u0026#39;float32\u0026#39;) diff = tf.pow(tf.subtract(y1, y2), 2.0) truth = tf.ones(diff.shape, dtype=\u0026#39;float32\u0026#39;) if T == 1 else tf.zeros(diff.shape, dtype=\u0026#39;float32\u0026#39;) return tf.pow(tf.sqrt(tf.tensordot(diff, truth, axes=len(diff.shape))), 2.0) tf.config.run_functions_eagerly(euclidean) Finally, add the training code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @tf.function def train(): for _ in range(10): index1 = random.randint(0, len(train_ds_arr)-1) index2 = random.randint(0, len(train_ds_arr)-1) T = 0 if index1 == index2 else 1 for (img1, label1), (img2, label2) in zip(train_ds_arr[index1].take(BATCH_SIZE), train_ds_arr[index2].take(BATCH_SIZE)): img1 = tf.expand_dims(img1, axis=0) / 255.0 img2 = tf.expand_dims(img2, axis=0) / 255.0 with tf.GradientTape() as tape: predictions = snn(img1, img2, training=True) loss = euclidean(predictions[0], predictions[1], T) gradients = tape.gradient(loss, snn.trainable_variables) optimizer.apply_gradients(zip(gradients, snn.trainable_variables)) print(loss.numpy) Here, we are randomly selecting a class from our master dataset array and — using a ternary — we compute whether the datasets are the same or different, which will be used as the truth value in the loss function. We select the batch and normalize the image values. Finally, we compute the forward pass and loss with GradientTape. The final lines compute the partial derivatives via automatic differentiation and apply these gradients to the optimizer.\nConclusion This shows just how simple and powerful Siamese Networks are. The networks are not just limited to images but can be applied to all forms of time-independent data that can be convolved. In fact, it is also possible to use Siamese networks without their convolutional component as simple Linear networks with a comparator (the loss function); however, it may function worse and is uncommon.\n","date":"2023-06-09T00:00:00Z","image":"https://thought.krishnashah.dev/p/what-is-a-siamese-network/cover_hu_148481c164c18747.webp","permalink":"https://thought.krishnashah.dev/p/what-is-a-siamese-network/","title":"What is a Siamese Network? (Implementation Included)"},{"content":"It is not what you think… We(specifically the Homo genus) have been on this beautiful planet for over 7000 millennia or 7 million years. For 95% of that time, we have intensively depended on our environment and local resources to thrive. Progress was slow, but so was the damage. We made tools out of rocks and sticks. A few million years later, we found brass, copper, iron, tin, etc., and life for all species on Earth was irreversibly altered. The avalanche was triggered on Everest, and even the most experienced mountain climbers can’t control it. Typically, these climbers would retreat to their base and allow the avalanche to pass, but their sheer confidence in their ability and self-proclaimed prowess in the mountains tells them to ignore the avalanche and reach the summit.\nWhat have we done? It took decades to foresee but visionaries, like Stephen Hawking, understood the ignored consequences of our careless actions. Most people only see the first 2 feet of the mile-deep hole we drilled. Contrary to popular opinion, it isn’t simply about resource depletion or environmental damage. While those are significant components, it isn’t the complete story. In fact, the most important factor has been erased from your brain by contemporary politics! The human mentality has collectively deteriorated beyond repair.\nPlease note: This next section is not a critique of particular countries. Applying these accusations to a particular state or nation is your opinion and not our words.\nReveal the truth… Capitalism, greed, money, and power have become the primary purpose of human existence. Doctors take their oath to help people, only to purposefully reject some people of their service for money. Politicians make extravagant claims, only to sit on power and money once their position is confirmed. People bear arms and kill innocent civilians inexplicably. Businesses never think twice when selling private user information, violating their privacy policy, to earn money. In the history of our world, knowledge has never had a price. Today, it costs more money than a house to acquire an education worthy of any value in this judgmental world. While a nation’s sole responsibility is to protect its citizens, today, they are enchanted by powerful weapons and make people pay huge sums of taxes and bills for life necessities like insurance and education. Nations who fought to eliminate taxes from their imperialistic parent country, are now imposing massive taxes and ignoring the financially impaired!\nWe like to think that we are smart, but our actions prove otherwise.\nWhat can you do? While these accusations were directed at countries and “more influential people”. This is, in fact, another pitfall of human mentality. Every person can make a difference, however, we choose not to. Speaking out about selfish actions, displaying kind human nature, and aiming to leave the world slightly better than you first saw it is the best thing you can do.\nWhy does it matter? — Butterfly Effect The butterfly effect is a branch of chaos theory aiming to show how an infinitesimal change in a deterministic state can result in drastic consequences in the future. Smiling and greeting a neighbor can prevent a murder! Publishing an article about the world can inspire others to rise and save our unrivaled blue planet.\n","date":"2022-06-15T00:00:00Z","image":"https://thought.krishnashah.dev/p/why-humanity-is-doomed/cover_hu_f8de7c39fdbcf4a6.webp","permalink":"https://thought.krishnashah.dev/p/why-humanity-is-doomed/","title":"Why Humanity is DOOMED!"},{"content":"The Premise It was the year 1980, and all software engineers worldwide were excited at a new frontier of technology: Artificial Intelligence. The ability for the “Perceptron” to be fed a bunch of images and teach itself to recognize different simple objects was revolutionary. Visionaries foresaw the full future dominated by this intelligence. As computing power grew, promptly with the advent of parallel computing and dense silicon chips, so grew people’s ambitions. And, from these ambitions came the inevitable question: Is it possible for an AI to think, learn, and react like a living thing, even better, a human?\nThe Concern As novels and movies came out, concern spread like wildfire. People shunned the idea of thinking computers, fearing the ultimate demise of the human race. In the arms race for intelligence, there is no room for error. As this concern grew, these enthusiastic engineers left this idea as a hopeless venture for the future, when the public opinion isn’t as harsh. As foreseeable, the opinion faded away, as people focused on more prevalent issues to riot over.\nThe Resurgence With renewed confidence and contemporary technology to bolster such progress, new AI research companies emerged, namely Deepmind and OpenAI. These companies often partnered with then-emerging tech giants like Intel, Google, Microsoft, IBM, and Nvidia. Powerful feats were achieved to demark the prowess of modern ingenuity and sophistication in algorithmic mathematics and AI. This field of study was suddenly more accepted by the community and was permitted to grow. Institutions around the globe researched new ways to automate their labor-intensive tasks. Convolutions, GANs, Transformers, and Markov chains soon flooded the market with opportunities. It was as if the collective field of AI grew in intelligence. As AIs performed better, and more efficiently, they also became more accessible. Companies incorporated these models in their advertisements, user management systems, and more.\nModern AI With the release of OpenAI’s GPT3 and the new almighty Deepmind Gato, people started claiming the invention of General Intelligence. People believe that General Intelligence is when an AI can do several, seemingly unrelated tasks, independently. Just because GPT3 can write code, draw pictures, and write completely original stories doesn’t mean that it is approaching the intelligence of a human being. Quite the contrary actually!\nWhat is General Intelligence? Throughout the years, there are several non-intuitive definitions made. True general intelligence is the ability of a machine to automatically acquire and train itself on a vast variety of knowledge that it may acquire from any source. It must also be able to make new predictions on the data upon inquisition (direct or indirect). This machine may not start with any pre-trained models, data, etc. OpenAI and Deepmind have made incredible progress and their feats are irrefutable evidence of innovation. But, they were trained on carefully procured data made by humans. This is what disqualifies it from the most comprehensive and logical definition of General Intelligence.\n","date":"2022-06-08T00:00:00Z","image":"https://thought.krishnashah.dev/p/what-really-is-general-intelligence/cover_hu_2dbffd8eb6b1219a.webp","permalink":"https://thought.krishnashah.dev/p/what-really-is-general-intelligence/","title":"What REALLY is General Intelligence?"},{"content":"What is Robotics? Robotics, put simply, is the combination of electrical, software, and mechanical engineering to make tasks easier. But, the amount of robots being used in repetitive tasks in warehouses and factories is exponentially growing. This begs the question, will the rapid popularization of automation diminish the need for humans?\nWhy do People do These Jobs? Oftentimes, people are compelled to do these jobs due to their unfortunate economic conditions. Debt from school, high unemployment due to a desperate global economy, and low levels of education can all put people in these types of jobs. People need the money to survive and a spiraling concern is their ensuing replacement by machines.\nWhat Can Robots Currently Do? Unfortunately, the general public only knows a fraction of the true potential of robotics. Some AIs create perfectly synthetic art from a single sentence description. Others make hyper-realistic human faces and write code for you just from a description! Some robots mow your lawn, weld entire car frames together, and even make Falcon 9 rockets that go to space. AIs have been used to design and manufacture the very airplanes you fly in. The advertisements we see are custom generated for each user using cookies and digital biometrics.\nThe Light at the End of the Tunnel! Despite the insane advancements in robotics, a technological revolution akin to this one has happened before. The industrial revolution, in particular, the advent of the steam engine sharply reduced the need for physical labor. The cotton gin and the mechanized loom left hundreds of thousands unemployed, and because of this people found new jobs to maintain the machines. In fact, this drastically increased the mean education level of people around the world.\nThe Future Outlook Despite the ominous clouds that hover over the future of AI and jobs, the grim attitude towards robotics research may be unnecessary. The next 50 years will be economically uninviting, the future of robotics will promote higher education, and innovation, and provide people with jobs that require human intelligence.\n","date":"2022-04-26T00:00:00Z","image":"https://thought.krishnashah.dev/p/will-automation-take-your-job/cover_hu_c0056f54c2273a41.webp","permalink":"https://thought.krishnashah.dev/p/will-automation-take-your-job/","title":"Will Automation Take Your Job?"},{"content":"See here for animation\nRecap: What is a Service Remember from the first part of this series, services are a node that allows you to give and input to get the desired output. If we want to add values, we can make a service that takes two numbers, sums them, and responds with the sum. Remember, they don’t publish continuously. They only respond when the service is called.\nAll good? Time to serve some nodes!\nFormat of this and future articles: It is important to understand how this article will go. We will go through an example together. I believe that knowledge can only be attained through practical examples.\nTask 1 Take in 2 numbers and respond with the product of those numbers.\nProcess A node will programmatically call the service and we must do the following:\nGet inputs Compute product Respond via topic the product Programming Making a ROS Package First, we have to make a ROS package which will be our node. That node will contain the code to make a topic and publish the message. There is a built-in command to do this using catkin.\nFirst, though, navigate to the workspace:\n1 cd catkin_ws/src NOTE: You MUST be in src.\nThen run this:\n1 catkin_create_pkg basic_ros_service std_msgs rospy roscpp --pkg_version 0.0.1 --description \u0026#34;Basic ROS Service\u0026#34; --license MIT --author \u0026#34;Krishna\u0026#34; --maintainer \u0026#34;Krishna\u0026#34; --rosdistro \u0026#34;noetic\u0026#34; Yikes! That is a lot so let me break it down.\nFirst, we have catkin_create_pkg which is the base command. Next comes the name of the package, in this case basic_ros_service. This can be anything. Then, we add all the dependencies of the package. Here, we have 3: std_msgs rospy roscpp std_msgs is a ROS package that contains a bunch of data types like Float64, String, Int64, and more! rospy is a library that contains methods for using ROS with Python. This is what we will use today. roscpp is a library that contains methods for using ROS with C++. We will NOT use this today. rosdistro states the distribution of ROS we are using. In this case, it is ROS “noetic”. (This is optional as long as you have sourced your ROS environment) Note: Services have additional requirements that we will manually add later! Everything that is mandatory to install has been done. Now on to the optional stuff.\nThese parts are really self-explanatory. It includes the license, description, author, maintainer, and version. You don’t need this but it is a good practice to keep. Now, list the files in the directory. You will see this:\n1 basic_ros_service CMakeLists.txt Go to basic_ros_service and list again.\n1 CMakeLists.txt include package.xml src CMakeLists.txt — contains all the info catkin needs to compile the project. It contains a list of the libraries, files, packages, and other stuff it has to compile. We will come back to it later. package.xml — contains info like package owner, description, version, maintainer, license, etc. We filled some of it earlier. The only important thing in it is the dependencies definitions. Our command already did this previously. It looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ***A BUNCH OF OTHER STUFF*** \u0026lt;!-- The *depend tags are used to specify dependencies --\u0026gt; \u0026lt;!-- Dependencies can be catkin packages or system dependencies --\u0026gt; \u0026lt;!-- Examples: --\u0026gt; \u0026lt;!-- Use depend as a shortcut for packages that are both build and exec dependencies --\u0026gt; \u0026lt;!-- \u0026lt;depend\u0026gt;roscpp\u0026lt;/depend\u0026gt; --\u0026gt; \u0026lt;!-- Note that this is equivalent to the following: --\u0026gt; \u0026lt;!-- \u0026lt;build_depend\u0026gt;roscpp\u0026lt;/build_depend\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;exec_depend\u0026gt;roscpp\u0026lt;/exec_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use build_depend for packages you need at compile time: --\u0026gt; \u0026lt;!-- \u0026lt;build_depend\u0026gt;message_generation\u0026lt;/build_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use build_export_depend for packages you need in order to build against this package: --\u0026gt; \u0026lt;!-- \u0026lt;build_export_depend\u0026gt;message_generation\u0026lt;/build_export_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use buildtool_depend for build tool packages: --\u0026gt; \u0026lt;!-- \u0026lt;buildtool_depend\u0026gt;catkin\u0026lt;/buildtool_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use exec_depend for packages you need at runtime: --\u0026gt; \u0026lt;!-- \u0026lt;exec_depend\u0026gt;message_runtime\u0026lt;/exec_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use test_depend for packages you need only for testing: --\u0026gt; \u0026lt;!-- \u0026lt;test_depend\u0026gt;gtest\u0026lt;/test_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use doc_depend for packages you need only for building documentation: --\u0026gt; \u0026lt;!-- \u0026lt;doc_depend\u0026gt;doxygen\u0026lt;/doc_depend\u0026gt; --\u0026gt; \u0026lt;buildtool_depend\u0026gt;catkin\u0026lt;/buildtool_depend\u0026gt; \u0026lt;build_depend\u0026gt;roscpp\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;rospy\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;std_msgs\u0026lt;/build_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;roscpp\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;rospy\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;std_msgs\u0026lt;/build_export_depend\u0026gt; \u0026lt;exec_depend\u0026gt;roscpp\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;rospy\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;std_msgs\u0026lt;/exec_depend\u0026gt; \u0026lt;!-- The export tag contains other, unspecified, tags --\u0026gt; \u0026lt;export\u0026gt; \u0026lt;!-- Other tools can request additional information be placed here --\u0026gt; \u0026lt;/export\u0026gt; \u0026lt;/package\u0026gt; That bottom part with the buildtool_depend and build_depend statements define the important libraries.\nFirst, we must make a folder in our package called srv/. This will hold our custom service. In it make a file called Product.srv. Put this in it:\n1 2 3 4 float64 a float64 b --- float64 product The first 2 float statements are the input. Then we put --- to show ROS that we are now defining the output. Lastly, we have the output. This allows us to define custom services with custom inputs and outputs.\nNavigate to the src within that. Now you should be in USERNAME/catkin_ws/src/PACKAGE_NAME/src.\nHere is where we put all of our code! Make a file called ProductServer.py. Again, names aren’t super important.\nHere is your file structure right now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 catkin_ws - build - devel - src - basic_ros_service - CMakeLists.txt - package.xml - include {This directory contains nothing} - CMakeLists.txt - src - ProductServer.py - srv - Product.srv Now let’s start on the code itself. First, you have to import the rospy and std_msgs.\n1 2 3 4 5 #!/usr/bin/env python3 from __future__ import print_function from basic_ros_service.srv import Product, ProductResponse # Import the compiled version of our service. There aren\u0026#39;t files called ProductResponse but ROS will understand that. import rospy import math Now we make the function that serves the clients.\n1 2 3 4 5 6 7 8 9 10 11 def product_func(a, b): # Actual multiplication function return a*b def compute(req): # Find the product from the inputs req.a and req.b and return the product. product = product_func(req.a, req.b) print(\u0026#34;Returning [%s]\u0026#34; % (product)) return ProductResponse(product) def product_server(): # Start the node and server. rospy.init_node(\u0026#39;basic_ros_service\u0026#39;) s = rospy.Service(\u0026#39;basic_ros_service\u0026#39;, Product, compute) print(\u0026#34;Ready to compute.\u0026#34;) rospy.spin() And then we can run it with this:\n1 2 if __name__ == \u0026#34;__main__\u0026#34;: product_server() IMPORTANT: You much make sure that your Python file is executable. sudo chmod +x ProductServer.py\nGo the CMakeLists.txt and add our python script to the file. This will make sure that catkin will compile it.\n1 2 3 4 5 include_directories( # include ${catkin_INCLUDE_DIRS} src/ProductServer.py ) Then, we need to make our SRV file that also needs to be compiled. Add the following lines to your CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 ## Generate services in the \u0026#39;srv\u0026#39; folder add_service_files( FILES Product.srv )## Generate added messages and services with any dependencies listed here generate_messages( DEPENDENCIES std_msgs ) Also, add a dependency to the find_package section:\n1 2 3 4 5 6 find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation ) We also need to add this dependency to our package.xml. Add the following lines next to all your other depend statements:\n1 2 \u0026lt;build_depend\u0026gt;message_generation\u0026lt;/build_depend\u0026gt; \u0026lt;exec_depend\u0026gt;message_runtime\u0026lt;/exec_depend\u0026gt; Now go back to catkin_ws. Run catkin_make. Source the directory source devel/setup.bash.\nIf it compiles, the hardest part is over!\nRunning it Start roscore: roscore. Now, start serving: rosrun basic_ros_service ProductServer.py.\nCheck the node using rosnode list. We will see our basic_ros_service service.\nCheck the service using rosservice list. We will see our basic_ros_service service and some logger services. Let us try calling our service using CLI.\n1 2 rosservice call /basic_ros_service \u0026#34;a: 2.0 b: 2.0\u0026#34; Hint: If you forget the format of the command hit TAB twice and it will auto-populate the command.\nWe will see our result of 4.0.\nReceive Data Programmatically Import libraries:\n1 2 3 #!/usr/bin/env python3from __future__ import print_functionimport sys import rospy from basic_ros_service.srv import * Define client function and make the appropriate call:\n1 2 3 4 5 6 7 8 def product_client(a, b): rospy.wait_for_service(\u0026#39;basic_ros_service\u0026#39;) # Find our service try: solver = rospy.ServiceProxy(\u0026#39;basic_ros_service\u0026#39;, Product) # Connect to the service resp1 = solver(a, b) # Make a request return resp1.product # Return the result except rospy.ServiceException as e: print(\u0026#34;Service call failed: %s\u0026#34;%e) Make the usage helper in case we forget how to use it:\n1 2 def usage(): return \u0026#34;%s [a b]\u0026#34;%sys.argv[0] Run the functions from the CLI arguments:\n1 2 3 4 5 6 7 8 9 if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) == 3: a = float(sys.argv[1]) b = float(sys.argv[2]) else: print(usage()) sys.exit(1) print(\u0026#34;Requesting %s*%s\u0026#34;%(a, b)) print(\u0026#34;%s * %s = %s\u0026#34;%(a, b, product_client(a, b))) Again, remember to change the permissions of the file. And run it the same way as the publisher.\nTo run it, do rosrun basic_ros_service ProductClient.py 2 2.\nConclusion In this article, you learned how to make a Service Server and Client. You also learned how to make custom messages in the form of service. Lastly, you dived a little deeper into the CMakeLists and the message_generation library. If you encounter any errors or have any questions, leave a comment. I will try my best to help you. I hope you enjoyed it. If there is a particular topic in ROS that you want me to cover, let me know in the comments as well!\n","date":"2022-04-01T00:00:00Z","image":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-4/cover_hu_7b4ebde700cbaa93.png","permalink":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-4/","title":"Beginner’s Guide to ROS — Part 4 “Services In-Depth” Tutorial"},{"content":"The Education System Many believe that math is all about 1+2 and a+b and just a conglomeration of random facts and virtual things meant to waste students’ time. Furthermore, they question why study chemistry, physics, biology, etc. And every person in the modern education system will likely agree.\nThe fact is that the modern education system is based on a few hundred-year-old, textbook-based lifestyle, which doesn\u0026rsquo;t adequately suit the mindset of the generation today. Many things aren’t told to “prevent confusion” or “make it easier” that certainly worked 200 years ago, but have an adverse effect today. Today, they make students study and memorize equations, memorize names, dates, and places. They “teach” students formulas that look foreign to students’ naive eyes. And even though there is a beautiful reason, proof, explanation, or logic behind every single one of these topics, it has been concealed in the interest of time and money.\nHigher precedence is given to tests and quizzes than to the acquisition of knowledge and understanding. Although tests are important to assess the depth of a student’s knowledge of a topic, there is a lack of coordination among teachers. This tessellates testing schedules such that students have tests and quizzes almost every single day. What purpose do they serve in teaching the student the beauty of this planet? Isn’t the true purpose of education to demystify the intricate mysteries this world possesses? Or is it to assess students, rank them, and sort them into groups based on their ability to persevere and sustain the pain these tests impose?\nThe Promise What if there was a world where the education system was a flexible string, on which students can gain experience, learn, and appreciate the world? What if the system was dedicated to finding the true nature of every student? Every person has a skill that they excel at; and if this skill is found early, the student greatly benefits from the fruits a hobby has to offer. If not, the student is forced to pick a stream of their choice after 4 years of general studies like mathematics, reading, science, and history. Imagine if each student was passionate about a subject so much that they would spend all of their free time on it? This would, of course, be in addition to the traditional courses taught intuitively.\nI believe that it is the education system’s job to assist students in discovering their passion for a field of study. This will exponentially increase the skill-set in the workforce and provide the world an avenue to rapidly expand and innovate. Instead of working in jobs, why can’t people make discoveries, find problems and solutions, and have an urge to help society? Isn’t that the true purpose of education? Isn’t that the purpose of humanity?\nReal Education However, I am not saying that what students learn in the education system is incorrect. Quite the opposite actually. I think that the education system is teaching the correct subjects and topics, but simply in the wrong fashion.\nMathematics in the real world I am sure you remember learning about hyperbolic functions like sinh, cosh, and tanh. I am sure you found them completely unrelatable and useless. But look at the images below:\nDo you notice a similarity? Some engineers wanted to ensure that a hanging bridge is safe. So, they tried to model a bridge, hanging on ropes, using a curve. They could NOT find a single curve that fits the bridge. Only when they found hyperbolic functions were they able to perfectly model the bridge. In fact, any flexible material suspended in the air by 2 points follows this curve!\nTurns out that hyperbolic functions are everywhere in nature. Bubbles always try to reduce their surface area. They follow the catenary curve(cosh(x))! These curves are now used anywhere that requires string or optimized shapes. Any energy-optimization, area-optimization, etc. requires these functions.\nImprovements Hobby Searching The schools should assist students to find their hobbies and teach them the general curriculum in a more intuitive way. This is the only way to cultivate a strong and specialized community that can advance our civilization.\nCurriculum They should explain the proofs, logic, and implications of everything students learn. They should teach us the logic, not the rules. Students should learn the history of mathematics and science. Students should be taught the genesis of each formula, theorem, and concept. Only when students understand why they are learning these topics, will they truly understand the meaning behind them. After that, there will be no need for studying or preparation, everyone will intrinsically understand the lessons. They will see applications of it anywhere.\nConclusion The modern education system consists of antique courses and teaching methods from the Industrial Revolution. This doesn’t benefit the more curious, modern generation. I would like you to contemplate today; when you were in the schooling system, were these your opinions too?\n","date":"2022-03-25T00:00:00Z","image":"https://thought.krishnashah.dev/p/blight-upon-humanity-the-education-system/cover_hu_2f44bde67642fc6b.webp","permalink":"https://thought.krishnashah.dev/p/blight-upon-humanity-the-education-system/","title":"Blight Upon Humanity: The Education System"},{"content":"Recap: What is a Topic Remember from the first part of this series, topics are a channel(a pipe), through which communication between nodes(sets of code) occurs. If you want to send sensor data from a microcontroller on a robot to another node on another machine(note how a machine can contain SEVERAL nodes), this would be the way to go.\nAlso, recall how a node can publish a topic and push data to it. Any other node on the network can anonymously subscribe to the topic and digest the information it can provide.\nAll good? Right then, let\u0026rsquo;s get out hands dirty with some code and some errors as well!\nFormat of this and future articles: It is important to understand how this article will go. We will go through an example together. I believe that knowledge can only be attained through practical examples.\nTask 1 We have a gyroscope on the robot that is publishing data in this format:\n1 2 3 Data: 135.6, 13.4, 3.14159 Format: FLOAT64 FLOAT64 FLOAT64 Axis: roll pitch yaw NOTE: The Format and Axis are NOT part of the data. They are just extra information I added.\nData Formatting We must first consider the format and structure of the data. This is not multidimensional data or a specialized format like an Image or Audio sample. This makes our life very easy.\nNext, we must consider the amount of data and its relationship. The roll, pitch, and yaw axes correspond to each other so it is impractical to send them individually like this:\n1 2 3 135.6 13.4 3.14159 ROS has the functionality to make a custom topic type that incorporates 3 Float64 in a single message. But, that is way more complicated and beyond the scope of this article(there may be an article on this soon though).\nWe can also make this a comma-separated string. This way, we can send a single String message that contains the following information:\n1 \u0026#34;135.6, 13.4, 3.14159\u0026#34; The subscriber can then easily splice the string and extract their information. That won’t be too intensive and still maintain the time-dependent structure of the data.\nProgramming Making the ROS Package First, we have to make a ROS package which will be our node. That node will contain the code to make a topic and publish the message. There is a built-in command to do this using catkin.\nFirst, though, navigate to the workspace:\n1 cd catkin_ws/src NOTE: You MUST be in src.\nThen run this:\n1 catkin_create_pkg basic_ros_topic std_msgs rospy roscpp --pkg_version 0.0.1 --description \u0026#34;Basic ROS Topic\u0026#34; --license MIT --author \u0026#34;Krishna\u0026#34; --maintainer \u0026#34;Krishna\u0026#34; Yikes! That is a lot so let me break it down.\nFirst, we have catkin_create_pkg which is the base command. Next comes the name of the package, in this case basic_ros_topic . This can be anything. Then, we add all the dependencies of the package. Here, we have 3: std_msgs rospy roscpp stg_msgs is a ROS package that contains a bunch of data types like Float64, String, Int64, and more! rospy is a library that contains methods for using ROS with Python. This is what we will use today. roscpp is a library that contains methods for using ROS with C++. We will NOT use this today. On a side note: ROS also supports Java, Lisp, and some other languages too! Everything that is mandatory to install has been done. Now on to the optional stuff.\nThese parts are really self-explanatory. It includes the license, description, author, maintainer, and version. You don’t need this but it is a good practice to keep. Now, list the files in the directory. You will see this:\n1 basic_ros_topic CMakeLists.txt Go to basic_ros_topic and list again.\n1 CMakeLists.txt include package.xml src CMakeLists.txt — contains all the info catkin needs to compile the project. It contains a list of the libraries, files, packages, and other stuff it has to compile. We will come back to it later. package.xml — contains info like package owner, description, version, maintainer, license, etc. We filled some of it earlier. The only important thing in it is the dependencies definitions. Our command already did this previously. It looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ***A BUNCH OF OTHER STUFF*** \u0026lt;!-- The *depend tags are used to specify dependencies --\u0026gt; \u0026lt;!-- Dependencies can be catkin packages or system dependencies --\u0026gt; \u0026lt;!-- Examples: --\u0026gt; \u0026lt;!-- Use depend as a shortcut for packages that are both build and exec dependencies --\u0026gt; \u0026lt;!-- \u0026lt;depend\u0026gt;roscpp\u0026lt;/depend\u0026gt; --\u0026gt; \u0026lt;!-- Note that this is equivalent to the following: --\u0026gt; \u0026lt;!-- \u0026lt;build_depend\u0026gt;roscpp\u0026lt;/build_depend\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;exec_depend\u0026gt;roscpp\u0026lt;/exec_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use build_depend for packages you need at compile time: --\u0026gt; \u0026lt;!-- \u0026lt;build_depend\u0026gt;message_generation\u0026lt;/build_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use build_export_depend for packages you need in order to build against this package: --\u0026gt; \u0026lt;!-- \u0026lt;build_export_depend\u0026gt;message_generation\u0026lt;/build_export_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use buildtool_depend for build tool packages: --\u0026gt; \u0026lt;!-- \u0026lt;buildtool_depend\u0026gt;catkin\u0026lt;/buildtool_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use exec_depend for packages you need at runtime: --\u0026gt; \u0026lt;!-- \u0026lt;exec_depend\u0026gt;message_runtime\u0026lt;/exec_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use test_depend for packages you need only for testing: --\u0026gt; \u0026lt;!-- \u0026lt;test_depend\u0026gt;gtest\u0026lt;/test_depend\u0026gt; --\u0026gt; \u0026lt;!-- Use doc_depend for packages you need only for building documentation: --\u0026gt; \u0026lt;!-- \u0026lt;doc_depend\u0026gt;doxygen\u0026lt;/doc_depend\u0026gt; --\u0026gt; \u0026lt;buildtool_depend\u0026gt;catkin\u0026lt;/buildtool_depend\u0026gt; \u0026lt;build_depend\u0026gt;roscpp\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;rospy\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;std_msgs\u0026lt;/build_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;roscpp\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;rospy\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;std_msgs\u0026lt;/build_export_depend\u0026gt; \u0026lt;exec_depend\u0026gt;roscpp\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;rospy\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;std_msgs\u0026lt;/exec_depend\u0026gt; \u0026lt;!-- The export tag contains other, unspecified, tags --\u0026gt; \u0026lt;export\u0026gt; \u0026lt;!-- Other tools can request additional information be placed here --\u0026gt; \u0026lt;/export\u0026gt; \u0026lt;/package\u0026gt; That bottom part with the buildtool_depend and build_depend statements define the important stuff.\nNavigate to the src within that. Now you should be in USERNAME/catkin_ws/src/PACKAGE_NAME/src.\nHere is where we put all of our code! Make a file called GyroPublisher.py. Again, names aren’t super important.\nHere is your file structure right now:\n1 2 3 4 5 6 7 8 9 10 11 catkin_ws - build - devel - src - basic_ros_topic - CMakeLists.txt - package.xml - include {This directory contains nothing} - CMakeLists.txt - src - GyroPublisher.py Now let’s start on the code itself. First, you have to import the rospy and std_msgs.\n1 2 #!/usr/bin/env python3import rospy from std_msgs.msg import String Now we make the function that publishes the actual messages.\n1 2 3 4 5 6 7 8 9 10 11 12 def publisher(): publisher = rospy.Publisher(\u0026#39;gyro_rpy\u0026#39;, String, queue_size=10) # Make a publisher with a topic called \u0026#34;gyro_rpy\u0026#34; with the type String. queue_size is the max amount of messages that can be queued. rospy.init_node(\u0026#39;GyroData\u0026#39;, anonymous=True) # Make a node called \u0026#34;GyroData\u0026#34;. Anonymous means that the name will be \u0026#34;GyroData\u0026#34; with a lot of random numbers. rate = rospy.Rate(10) # 10Hz while not rospy.is_shutdown(): # Run forever until exit. roll = 135.6 # set values pitch = 13.4 # set values yaw = 3.14159 # set values gyro_mock_data = f\u0026#34;{roll}, {pitch}, {yaw},\u0026#34; # Combine the strings together rospy.loginfo(gyro_mock_data) # log the Publisher data publisher.publish(gyro_mock_data) # Publish rate.sleep() # Slow down the publishing to not choke CPU. And then we can run it with this:\n1 2 3 4 5 6 if __name__ == \u0026#39;__main__\u0026#39;: try: publisher() except rospy.ROSInterruptException: print(\u0026#34;Publisher Interrupted...\\nExiting cleanly...\u0026#34;) exit() IMPORTANT: You much make sure that your Python file is executable. sudo chmod +x GyroPublisher.py\nGo the CMakeLists.txt and add our python script to the file. This will make sure that catkin will compile it.\n1 2 3 4 5 include_directories( # include ${catkin_INCLUDE_DIRS} src/GyroPublisher.py ) Now go back to catkin_ws. Run catkin_make. Source the directory source devel/setup.bash.\nRunning the Node Start roscore: roscore. Now, start publishing: rosrun basic_ros_topic GyroPublisher.py.\nCheck the node using rosnode list. We will see our GyroData node with a bunch of numbers since it is anonymous.\nCheck the topic using rostopic list. We will see our gyro_rpy topic. We can actually see the data using rostopic echo /gyro_rpy:\n1 2 3 4 5 6 7 8 9 data: \u0026#34;135.6, 13.4, 3.14159,\u0026#34; --- data: \u0026#34;135.6, 13.4, 3.14159,\u0026#34; --- data: \u0026#34;135.6, 13.4, 3.14159,\u0026#34; --- data: \u0026#34;135.6, 13.4, 3.14159,\u0026#34; --- data: \u0026#34;135.6, 13.4, 3.14159,\u0026#34; Receive Data Programmatically Import libs:\n1 2 3 #!/usr/bin/env python3 import rospy from std_msgs.msg import String Define callback to log:\n1 2 def callback(data): rospy.loginfo(rospy.get_caller_id() + f\u0026#34;Recieved Data: {data.data}\u0026#34;) # Log data Make the subscriber node:\n1 2 def sub(): rospy.init_node(\u0026#39;sub\u0026#39;, anonymous=True) # Initialize the noderospy.Subscriber(\u0026#34;gyro_rpy\u0026#34;, String, callback) # Start a subscriber. The name here is the name of the topic we want to subscribe to.rospy.spin() # Keep the node running until program exits. Run the script:\n1 2 if __name__ == \u0026#39;__main__\u0026#39;: sub() Again, remember to change the permissions of the file. And run it the same way as the publisher.\nConclusion In this article, you learned how to make Publishers and Subscribers in ROS. If you encounter any errors or have any questions, leave a comment. I will try my best to help you. I hope you enjoyed it. If there is a particular topic in ROS that you want me to cover, let me know in the comments as well!\n","date":"2022-03-18T00:00:00Z","image":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-3/cover_hu_be03e76db2ffe10a.png","permalink":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-3/","title":"Beginner’s Guide to ROS — Part 3 “Topics In-Depth” Tutorial"},{"content":"It is fair to say that humans are a dominating species on our blue planet. And certainly, the question must have crossed your mind at some point, “Why humans?” In retrospect, we make a mess wherever we go. The Great Garbage Patch, global warming, mass hunting and extinction, habitat loss, and there are several more. But that is not all! We savagely kill each other and fight for the most insignificant things. We are selfish, greedy, and vengeful. In fact, we eradicated several other species of humans like the Neanderthals.\nIntelligence The standard argument that everyone convinces themselves with is, “We are smarter.” There are several flaws with this. What is smartness? If it is defined by multidimensional thinking, many animals like whales and dolphins beat us. This is because they have larger brains with more gyri(folds in the brain) that increase spatial thinking abilities.\nIs it memory? Nope. Chimpanzees have a significantly better memory than us.\nIs it language? Evidently not. Parrots can comprehend and retain languages extremely quickly. And wolves communicate and coordinate attacks in seamless precision. It can be compared to a special ops military attack.\nA critic might propose that we are simply the culmination of all these animals and that is why we dominate. However, the octopus contains all the traits mentioned above.\nOctopii Octopuses have the perfect proportion of memory, recognition, spatial thinking, pattern recognizing, agility, strength, etc. While we have roughly 60 protocadherins, octopuses have approximately 168! This means that they have 3 times the neural wiring capacity as us humans. Then why don’t we see octopuses making mansions and instruments and conducting mathematics?\nThe Hard Truth We humans have no idea. If octopuses desired, they could certainly advance to our level and challenge the human species. They have already done numerous feats like solving Rubik\u0026rsquo;s cubes, solving other puzzles, and using mechanical advantage in their tools. They have all the resources we have on land, including minerals, chemicals, precious metals, and plastic bags(thanks to us). Then why do we see octopuses still using snail shells to protect themselves? Why haven’t octopuses created utopias and colonies underwater? Why can’t they make themselves a palace of coral and live in peace?\nThe Difference As enunciated previously, we are greedy. We don’t care about sustainability or the environment we depend on. We care about self-preservation and comfort. Maybe, the octopuses have realized the importance of nature. Octopuses have existed on Earth for 500 MILLION years as opposed to us, amateurs, who have existed for a mere 6 million years. Conceivably from their previous experience, they have understood the harsh lessons we are only now realizing. Perhaps they changed their lifestyle before it was too late.\nThe Key Takeaway We NEED to learn from the octopuses. We MUST let go of our ego and learn from these wise and experience creatures that have been on this planet for magnitudes more than us. And it is not just the octopuses, but every animal on Earth. Humans are unarguably the most unsustainable species on Earth contrasting with almost all other animals.\nThere are many animals that can challenge the human race if they wished. But, they have more sustainable practices and goals than us.\nIf octopuses went through the scientific, technological, and economic advancements that we are going through, it would be impossible to prove millions of years later. The dynamic ocean would have concealed the slightest traces of technology.\nSo, while we think that octopuses, chimpanzees, goats, etc. are just animals, there is a motive to consider why that is?\n","date":"2022-03-12T00:00:00Z","image":"https://thought.krishnashah.dev/p/the-truth-about-humans/1_hu_31f85dbee90457b9.webp","permalink":"https://thought.krishnashah.dev/p/the-truth-about-humans/","title":"The Truth About Humans"},{"content":"We have all heard of or even discussed ethics before. Some may have even heard of a new field of philosophy called “AI Ethics.” But what is it really?\nAI Ethics This emerging field of philosophy deals with the primary question “If a person made a sentient machine or program, would shutting the program down or turning off the computer be equivalent to killing it?” Your immediate answer may be “No, it is just a computer that WE made!” But let me provide you with a series of questions that provide depth to this problem!\nIQ IQ is a general measure of multi-spatial thinking. If a robot has the IQ of a dog or cat, can’t we equate it to a dog or cat? But others might say that AI is not really thinking and is simply predicting from a defined set of data. But how do we know what humans or other animals are thinking? Just because we have a larger dataset and more processing power doesn’t mean we are thinking.\nMaterial Crisis Another justification to “pull the plug” might be to say that the AI is simply a bunch of transistors, plastic, and metal. But how can we consider humans superior to robots? After all, we are made of oxygen, hydrogen, nitrogen, and other elements. Our brain is simply a bunch of electrical signals(sounds familiar?), and our emotions are just specific chemical compounds released during specific events. Computers(frankly technology in general) are just a simple reflection of us and the way we think.\nShip of Theseus Problem Imagine you have a boat that is sailing a hundred miles. But, you couldn’t shell out enough shillings for a fancy-pants boat so you went to the dark alley at midnight and bought one from a guy in a hood. So along the way, your boat fell apart piece by piece and you summoned replacement parts to fix it. When it reached its destination, you ended up replacing EVERY single part of it. Is this still the same boat? If so, what if a storm destroyed the entire boat and you built it from scratch so it is identical to the original? Is it still the same boat then?\nThe scenario here is asking if it is the parts that define an object or the experience the boat went through. This matters because depending on the interpretation you take you could argue that if you turn off the AI, but turn on the exact same version later, you didn’t kill the ai. Now, you may have a completely different opinion about the Material Crisis. Now, it doesn’t matter if you are made of steel or cells, what matters is the experiences you make with it.\nSo What? What sets us(humans made of cells, flesh, and bone) from them(robots made of steel, motors, and a frame)? If there is no major difference setting us apart, then why would killing a sentient robot be justified? And if a robot has consciousness, should we consider it equal to us? After all, humans consider themselves superior to animals in many ways.\nSo is there just 1 answer? No, not really. What if an AI has the potential to be sentient but is still growing? Can we kill that? Just because it isn’t currently sentient doesn’t mean anything. Humans come from a single cell that is not technically sentient. But as the baby develops, it grows the ability to have sentience.\nThere are 3 frames of view:\nNo life matters and killing anything(human, animal, or robot) is okay. Some life matters(ethnocentric; humans matter but not animals and robots) All life matters and killing is NOT justified. Potential Ideas Battery What if we make a sentient AI and then let it run on battery power until it drains and dies. Technically, it lived its maximum lifespan and its “heart stopped beating”. But you, the inventor, had the ability to swap or attach another battery or an external power source. That is equivalent to a doctor giving a heart transplant. Would a doctor deny your transplant, would you?\nTeach Self-Termination Sequence What if the first thing you taught the AI is to terminate and wipe the OS on user command? That would be the equivalent of a human “committing suicide”. But, didn’t you “pressure” the robot to do it? Just because a hitman kills a person doesn’t mean that the employer(of the hitman) isn’t at fault.\nSoul Many may argue that a soul sets biological beings apart from robots. But, what is a soul? Who made it? Is it a subconscious state of the brain or a separate entity running our body? Or is a soul just something made up by humans to try to set them apart from everything else?\nConclusion Oftentimes, this decision involves religion and personal biases. But, it is crucial to think about these topics as General Intelligence may not be as far as we think, or hope!\n","date":"2022-02-16T00:00:00Z","image":"https://thought.krishnashah.dev/p/ai-ethics-philosophy/cover_hu_f2ef70cdb7b01cf.webp","permalink":"https://thought.krishnashah.dev/p/ai-ethics-philosophy/","title":"AI Ethics — Crucial Law or Theoretical Philosophy"},{"content":"Alright, we have all heard of ROS, the Robot Operating System. But frankly, we don’t know what it is. Is it an OS? After all, it is in the name. Here is where the tricky part starts. Contrary to its name, it is just a software package that allows you to compile code-stacks, use 3rd party code-stacks, and apply these code-stacks to a peer-to-peer communication system for robotics. That was a lot so let me break this down.\nTake an Example! You have a robot running some ARM device like a Raspberry Pi. It is a 3-wheeled robot and can’t do much complex processing. But you want to do some cool stuff like mapping and navigation. You want the robot to roam around and map the building. Once the map is ready, you want to tell the robot to go to the kitchen and it will follow.\nThis is a very complex problem to solve for a weak Raspberry Pi. But then, genius strikes! You also have a powerful PC with a GPU and all. What if the PC does all the hard stuff and sends all the results to the Pi? This is what ROS does(essentially).\nImportant Note What I am about to present from here on is strictly applicable to ROS1, not ROS2. ROS2 runs a more complex system that is drastically different from ROS1. I might make an article later for ROS2. What a standard ROS system looks like? Let us start at the top and work down! At the top is the master. This is running the code to handle all the messages from the various nodes. But what are these nodes sending?\nNodes So a Node is a big umbrella that can publish various topics, services, and actions. It could be the camera node that published images and can supply other info as a service. It can be any way you think you can logically organize your project.\nTopics Topics are what you think they are. They are names under which messages pass. The messages could be a String like, “Hi robot revolution!”, an Int like 42, or a custom one you made up like an image. For all we know, you can send any sort of data formats like arrays with images and strings, or a PointCloud, etc. These custom messages are made using .msg files. In it, you can specify the structure of your message and after compiling and a bit of coding, they can be used.\nServices A service is, well, a service. One node can request another node to do something based on data. I can give a node the numbers 2 and 3 and the node gives me back 5. Sorcery, I know! These work the same as messages in the way they are made. You would use a .srv file to make them.\nActions You tell the robot to go forward. But then you forget! You can request the action node to return a status. Wait, the robot is about to fall off a cliff! You can also cancel the action. It is more complex than this but this is the basics.\nAnyway, the master handles all the messages, services, and actions. A node is a community of code that contains topics, services, and actions. There are 2 things nodes can do.\nPublish Publishers publish data and don’t care about the subscriber. For all we know, the publisher could be publishing data to an alien from Mars.\nhttps://www.utahpeoplespost.com/wp-content/uploads/2015/11/NASA-Announces-Big-Reveal-About-Mars.-What-Is-It.jpg\nSubscribe https://cdn.dribbble.com/users/5528375/screenshots/14069669/subscribe_animated_gif.gif No not that subscribe. Subscribers subscribe to a publisher, hence start receiving the data being published. Again, the subscriber just knows the name of the publisher. The exchange is anonymous.\nROS CLI This is also a tricky part of ROS. There are several commands but here is how they work. You have a command for each of the main sections I talked about above.\n1 2 3 4 5 6 roscore - to start Master server. rosnode - to help control ROS nodes rostopic - to manage topics rosservice - to manage services rosaction - to manage action rosrun - this is the command we use the most. It is to run your code once it is compiled. You see how each of the commands is broken up into smaller sections based on different aspects of ROS. Note, that there are more commands than shown here. Each one of these commands has sub-commands. I will expand later in other parts on what they all do, but for now, know that you can read(echo) messages, run services and actions, measure the frequency and size of topics, and much more.\nRVIZ RVIZ is a lifesaver for us mere mortals. Only the ROS Gods can live without it. It is a visualization tool where any sort of topic, message, or info coming in or out can be seen. PointClouds, odometry, images, video, numbers, and more. It can be run by simple typing “rviz” or “rosrun rviz rviz” but I will get into that later.\nWhat is a compiler? We all know a compiler like g++ that takes C++ code and converts it to machine code that the computer better understands. Well, catkin(ROS compiler) is like g++ on steroids. It takes the code with some config information and compiles the entire node with respect to ROS libraries and other nodes. It can see the entire project and compile everything at once.\nhttps://media1.tenor.com/images/7eece0a4e5ecd2cb8b91a122d5d8efd1/tenor.gif?itemid=13161198\nPractice Question Can you compile Python?\nAnswer: Yes!\nAny programmer would say NO! But the magical catkin can! It compiles Python too. This way, ROS can use both Python and C++ together.\nI know the professional developers must be frowning down on me but this was a conceptual way to understand it. Don’t worry, I will explain later! ","date":"2021-11-08T00:00:00Z","image":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-1/cover_hu_c0bde90b71f778f5.png","permalink":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-1/","title":"Beginner's Guide to ROS — Part 1 “The Working Principle” Tutorial"},{"content":"Alright, be honest. We all were like, “ROS sounds cool.” Let me install it and see how it works, plus how hard can installing it be(famous last words). Then you search up “ROS install Ubuntu”, and you find 50 guides all saying something different. Some require 4 hours of compiling while others simply use ATP. What is the deal with all of this?\nMethod 1: Easy way out! https://imgr.search.brave.com/HrMbmi1kSMEnJflnt8rSZnsbBd8q3LKt_4gurC8rsos/fit/602/606/ce/1/aHR0cHM6Ly9pLnBp/bmltZy5jb20vb3Jp/Z2luYWxzL2Q5LzJm/L2RkL2Q5MmZkZGQ1/NGFkOTA3NTQ5NWY0/ODdjMTRhM2ZjODQ1/LmpwZw Now how this one works is simple. ROS only supports LTS versions of Ubuntu. That is 20.04, 18.04, 16.04, 14.04, etc. Now for each LTS version, there is a version of ROS. For 20.04, it is Noetic Ninjemys, for 18.04, it was Melodic Morenia, etc. You must be wondering, “Can you communicate between other versions of ROS?” Absolutely! They will communicate flawlessly with each other. They communicate over an IP Protocol that is common and reverse compatible. It is a little difficult to communicate between ROS1 and ROS2 since you must use ros_bridge. But that’s for another day!\nFor Ubuntu 20.04 (Noetic Ninjemys) Now, unless you have some crazy dependency, the latest ROS1 version is the way to go for beginners. The latest as of December 12, 2021, is ROS Noetic. Installing previous versions will be a little harder. 1 more thing, I will show installation on Ubuntu 20.04 but other operating systems like Arch, Windows, and Debian are available. Here is how you can install Noetic Ninjemys:\nLink to other OS installations: http://wiki.ros.org/noetic/Installation\n1 2 3 4 5 6 7 8 9 10 11 12 13 (Usually done by default) Make sure \u0026#34;restricted,\u0026#34; \u0026#34;universe,\u0026#34; and \u0026#34;multiverse\u0026#34; are enabled. Check https://help.ubuntu.com/community/Repositories/Ubuntu sudo sh -c \u0026#39;echo \u0026#34;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/ros-latest.list\u0026#39; # Add their repository to your machine sudo apt install curl # Usually comes preinstalled curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - # Add repository key sudo apt update # Update repository listing sudo apt install ros-noetic-desktop-full # Contains all essential packages like perception and simulation. Perfect for beginers. Others are \u0026#34;ros-noetic-ros-base\u0026#34; which is the bare-bones. The middle version is \u0026#34;ros-noetic-desktop\u0026#34; which had some debug tools but none of the perception and simulation packages. You must source the ROS environment everytime so you can run \u0026#34;source /opt/ros/noetic/setup.bash\u0026#34; but if you don\u0026#39;t want to run it everytime you spawn a terminal echo \u0026#34;source /opt/ros/noetic/setup.bash\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # Close all terminal instances and reopen terminal. # Update ROS Core Dependencies: sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential # Install tools sudo apt install python3-rosdep # Install rosdep(tool) sudo rosdep init # Initialize rosdep rosdep update # Update rosdep That’s it!\nMethod 2: From Source (Advanced Users) There is a separate way where you can install and compile all the ~150 packages from source. This is for the professional developers or the confused beginners(get it?). Please let me know in the comments if you want me to make a separate article on this!\nSetup Development Environment Alright, the installation is finally complete. Not so fast! We still have to make the development environment. Don’t worry, it is SUPER easy!\n1 2 3 4 mkdir WORKSPACE_NAME # Go to the place you want your code, environment, and packages to reside. Replace WORKSPACE with a name of your choice. cd WORKSPACE_NAME # Go into directory mkdir src # DO NOT change this name or it won\u0026#39;t compile. Also, DO NOT cd into it catkin_make # compile your blank project! Take if for a Spin! Finally, the moment you have all been waiting for…your very first ROS1 command!\nNote: Please note: this article shows you how to set up ROS on 1 computer. Meaning that the Master and Nodes are on the same computer. TO communicate across many computers, there is a painful setup procedure that I won’t make you endure just yet(brace yourself for the future).\nROS Core ROS1(not ROS2) works on the following system: ROS Core manages the master node. It manages all the communication happening in the ROS network. You cannot run any ROS nodes without it.\n1 roscore # Spawns a ROS master node. Turtle Sim The first thing you are going to do is make a little turtle draw things on a screen using your arrows.\nIf you are short on time, install it using APT:\n1 sudo apt-get install ros-$(rosversion -d)-turtlesim # install the turtle sim package If you want to learn ROS properly, do this:\n1 2 3 4 5 6 cd WORKSPACE_NAME/src # Go to the source directory sudo apt install git # Install git if you don\u0026#39;t have it. git clone https://github.com/ros/ros_tutorials.git # Download the turtle_sim and lesson packages cd .. # Go to main directory catkin_make # Compile the project source devel/setup.bash # Re-Source compiled files. Now, let us run it! Make sure roscore is running on a different terminal\n1 2 rosrun turtlesim turtlesim_node # command node script This should show a box where you can move a turtle around. But more importantly, is the things happening behind the scene.\n1 rosnode list What do you see? Can you find our node?\n1 rostopic list Where is the turtle location topic? Did you see it?\n1 rostopic \u0026lt;TURTLE_POSE_TOPIC_NAME\u0026gt; echo Replace the first argument with the topic name you previously found. Can you see the coordinates move as the turtle runs around(or should I say crawl around)?\nThings to note: cmd_vel: This is what we call a Twist message. It contains target velocity values for 3 axes of translation and 3 axes of rotation. You can imagine how useful this is for a remote-controlled robot.\nCommand structure: Did you notice the pattern in the commands. It is the object you are accessing(rosnode, rosservice, rostopic, rosrun, etc), then the action you want to perform(echo, list, hz, etc), then the arguments. We will dive into the depth of these commands in another article.\n","date":"2021-11-08T00:00:00Z","image":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-2/cover_hu_55283b27a8d55613.jpg","permalink":"https://thought.krishnashah.dev/p/ros-beginners-guide-part-2/","title":"Beginner’s Guide to ROS — Part 2 “Installation and First Run” Tutorial"},{"content":"When it comes to AI or robotics, the common public reaction is perceivable. When they talk about global domination by robots or job loss due to advances in robotics, their assumptions are highly based on science fiction and not that much on reality. In fact, robotics is an indistinguishable part of our lives and essential to us in a controlled manner. Whether you are strongly against anything related to robotics or are a lead engineer at Boston Dynamics, it is indisputable that robotics is in our everyday lives. Everything from a toaster, to your computer, there is some form of robotics in it. For example, while you are browsing on Amazon and it suggests you exactly what you are looking for, you witnessed AI. AI has just helped you and Amazon. After introducing this AI, Amazon saw a 29% increase in sales and $12.83 billion more dollars in revenue. This shows that AI is helping customers like you and companies like Amazon earn money. In fact, our lives are also at the hands of AI. Boeing and other companies are creating more efficient airplane air-frames using AI. They have trained the AI to take their requirements and it will design the most perfect air-frame.\nAccording to Boeing, their new Boeing 777 will be 100% computer generated and assembled online before they even bring out the materials! In fact, the computer will even create a special mixture of metals and composites to make the air-frame lighter! Look around, every item near you is probably manufactured in a factory. In the year 2020 alone, 1.64 million industrial robots are used in factories around the globe and analytics say that they will replace 20 million factory jobs by 2030! This raises the concern that our interdependence might lead to severe problems depicted by literature.\nThe ethical aspect of AI has been an extremely controversial topic. There is great hope that AI can personalize school learning plans, help disabled people, and revolutionize our lives, but there must be a balance. At a deeper level, AI is just a math equation that runs behind it but its ability to gradually learn is one fact that concerns many.\nThough, if AI can be used conservatively and concisely, the severity of the consequence may be minimal. That raises a question though; as human beings, we are very curious, and we will strive to learn more and eventually lead to problems much greater than our imagination. But how severe is severe?\nThat rhetorical question has quite a simple answer. AI is being developed in individual parts like some work on vision while others work on speech. Other engineers work on the robotic humanoid part. When a person gets the idea of combining all the parts, he/she will make a close replica of a human, and if not given the right knowledge of ethics and society, the robot will believe that it is one of us. As humans are quite violent, in the sense that we are always fighting each other, the robots will do the same. Many people discriminate, the robots will do the same. They will be our replica only made of metal and 100 times stronger. This is what we fear and if not controlled, we won’t have to watch movies to fear it.\nTo conclude, the question is not whether AI is ethical but rather where is the limit. We must learn to live within boundaries and know that exceeding them can have grave consequences. The research our scientists have done is truly incredible but only to a limit. The next time you pull out a mobile phone or any other device, remember our interdependence with it and its similarity to any other robot or AI.\n","date":"2021-04-29T00:00:00Z","image":"https://thought.krishnashah.dev/p/ai-problem-salvation/cover_hu_d909d9eb701c68f7.jpg","permalink":"https://thought.krishnashah.dev/p/ai-problem-salvation/","title":"AI - A Problem or Our Salvation"}]